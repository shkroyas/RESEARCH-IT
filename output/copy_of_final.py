# -*- coding: utf-8 -*-
"""Copy of final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s4v7K_U80i0t7tFrg2nQl4XtpdI_1Ayy

# LangChain with Gemini and RAG Implementation

A complete implementation of Retrieval Augmented Generation using Google's Gemini models.
"""

# Install required packages
from pyngrok import ngrok
import uvicorn
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from fastapi import FastAPI, Request
from langchain.schema import Document  # Add this import
from langchain.chains.llm import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, PDFPlumberLoader
from langchain_core.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from pypdf import PdfReader
import logging
from pprint import pprint
from pathlib import Path
import warnings
import os
import google.generativeai as genai
from IPython.display import display, Markdown
import re
import textwrap


# Suppress warnings
logging.getLogger('pypdf').setLevel(logging.ERROR)
warnings.filterwarnings("ignore")

"""## Helper Functions"""


def to_markdown(text):
    text = text.replace('â€¢', '  *')
    return Markdown(
        textwrap.indent(text, '> ', predicate=lambda _: True))


"""## Configure Gemini API"""

try:
    GOOGLE_API_KEY = 'AIzaSyA10ceOZNiCW-lEOYUWD3WARt-6EXjY4BA'
    genai.configure(api_key=GOOGLE_API_KEY)
    print("Gemini API configured successfully!")
except Exception as e:
    raise ValueError("Please set your GOOGLE_API_KEY in Colab secrets") from e

"""## Initialize Gemini Models"""

text_model = genai.GenerativeModel(model_name="gemini-1.5-flash")

"""## Text Generation Example"""


def generate_text(prompt):
    try:
        response = text_model.generate_content(prompt)
        return to_markdown(response.text)
    except Exception as e:
        return f"Error generating text: {str(e)}"


# Test text generation
display(generate_text("What are the use cases of LLMs?"))

"""## LangChain Integration"""


# Initialize LangChain LLM
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    google_api_key=GOOGLE_API_KEY,
    temperature=0.2,
    convert_system_message_to_human=True
)

"""## RAG Implementation"""


class RAGPipeline:
    def __init__(self, pdf_path=None):
        self.pdf_path = pdf_path
        self.vector_index = None
        self.qa_chain = None
        self.full_text = None

    def load_and_process_documents(self, pdf_path=None):
        """Load and split PDF documents"""
        path_to_use = pdf_path if pdf_path is not None else self.pdf_path
        if path_to_use is None:
            raise ValueError("No PDF path provided")

        try:
            print(f"Loading PDF from {path_to_use}...")
            loader = PyPDFLoader(path_to_use)
            pages = loader.load_and_split()

            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=10000,
                chunk_overlap=1000
            )

            self.full_text = "\n\n".join(str(p.page_content) for p in pages)
            texts = text_splitter.split_text(self.full_text)

            print("Creating embeddings...")
            embeddings = GoogleGenerativeAIEmbeddings(
                model="models/embedding-001",
                google_api_key=GOOGLE_API_KEY
            )

            self.vector_index = FAISS.from_texts(
                texts, embeddings).as_retriever(search_kwargs={"k": 5})
            print("Document processing complete!")
            return True
        except Exception as e:
            print(f"Error loading documents: {str(e)}")
            return False

    def _create_summary_chain(self):
        """Create a LangChain summarization chain"""
        summary_template = """Write a comprehensive summary of the following document with these EXACT section headers:

        1. **OVERVIEW** - 2-3 paragraph high-level summary
        2. **KEY FINDINGS** - Bullet points of main discoveries
        3. **METHODOLOGIES** - Approaches used in the research
        4. **RECOMMENDATIONS** - Suggestions for future work

        Document:
        {text}"""

        prompt = PromptTemplate.from_template(summary_template)
        llm_chain = LLMChain(llm=llm, prompt=prompt)
        return StuffDocumentsChain(
            llm_chain=llm_chain,
            document_variable_name="text"
        )

    def initialize_qa_chain(self):
        """Initialize QA chain with prompt template"""
        template = """Use the following pieces of context to answer the question at the end.
        If you don't know the answer, just say that you don't know, don't try to make up an answer.
        Keep the answer concise and accurate. Always end with "Thank you for your question!"

        Context: {context}
        Question: {question}
        Helpful Answer:"""

        QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

        self.qa_chain = RetrievalQA.from_chain_type(
            llm,
            retriever=self.vector_index,
            return_source_documents=True,
            chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
        )
        print("QA Chain initialized!")

    def query(self, question):
        """Query the RAG pipeline"""
        if not self.qa_chain:
            print("QA chain not initialized. Please call initialize_qa_chain() first.")
            return None

        try:
            print(f"\nQuestion: {question}")
            result = self.qa_chain.invoke({"query": question})
            display(to_markdown(result["result"]))
            return result
        except Exception as e:
            print(f"Error querying: {str(e)}")
            return None

    def generate_structured_summary(self):
        """Generate structured summary using LangChain's StuffDocumentsChain"""
        if not hasattr(self, 'full_text') or not self.full_text:
            print("Error: Document not loaded")
            return None

        try:
            # Create document from full text (first 50k chars to avoid token limits)
            docs = [Document(page_content=self.full_text[:50000])]

            # Create and run summary chain
            summary_chain = self._create_summary_chain()
            result = summary_chain.run(docs)

            # Initialize sections
            sections = {
                "overview": "",
                "key_findings": "",
                "methodologies": "",
                "recommendations": ""
            }

            # First try splitting by numbered sections
            parts = re.split(r'\n\d+\.\s+\*\*', result)
            if len(parts) >= 4:  # We expect at least 4 sections
                section_names = ['overview', 'key_findings',
                                 'methodologies', 'recommendations']
                for i, name in enumerate(section_names):
                    if i+1 < len(parts):
                        # Clean the content
                        content = parts[i+1]
                        # Remove the closing ** if present
                        content = content.replace('**', '')
                        # Remove any remaining section headers
                        content = re.sub(r'^\w+\s*\*\*.*\*\*',
                                         '', content).strip()
                        sections[name] = content

            # If that didn't work, try alternative parsing
            if not any(sections.values()):
                # Try splitting by bold headers
                parts = re.split(r'\*\*(\w+)\*\*:', result)
                for i in range(1, len(parts), 2):
                    section_name = parts[i].lower().strip()
                    if section_name in sections:
                        sections[section_name] = parts[i+1].strip()

            # Final fallback
            if not any(sections.values()):
                sections["overview"] = result

            return sections

        except Exception as e:
            print(f"Summary generation failed: {str(e)}")
            return None

    def generate_structured_summary(self):
        """Generate a comprehensive structured summary with detailed content"""
        if not hasattr(self, 'full_text') or not self.full_text:
            print("Error: Document not loaded")
            return None

        try:
            # First generate a full detailed summary
            summary_template = """Generate an EXTREMELY DETAILED structured summary of this document with these EXACT sections:

            1. **OVERVIEW** - Provide a comprehensive 3-4 paragraph summary covering:
               - Main purpose and objectives
               - Core thesis or argument
               - Key context and background
               - Overall significance

            2. **KEY FINDINGS** - List ALL important findings as:
               - Detailed bullet points (10-15 items)
               - Include specific data points where available
               - Note any surprising or counterintuitive results

            3. **METHODOLOGIES** - Describe ALL approaches used:
               - Research methods and techniques
               - Data collection procedures
               - Analysis frameworks
               - Any innovative methodologies

            4. **RECOMMENDATIONS** - Provide complete suggested actions:
               - Immediate next steps
               - Long-term proposals
               - Policy implications
               - Future research directions

            Document:
            {text}"""

            prompt = PromptTemplate.from_template(summary_template)
            llm_chain = LLMChain(llm=llm, prompt=prompt)
            summary_chain = StuffDocumentsChain(
                llm_chain=llm_chain,
                document_variable_name="text"
            )

            docs = [Document(page_content=self.full_text[:50000])]
            full_summary = summary_chain.run(docs)

            # Parse the detailed summary
            sections = {
                "overview": "",
                "key_findings": "",
                "methodologies": "",
                "recommendations": ""
            }

            # Enhanced parsing that handles complex formatting
            section_pattern = re.compile(
                r'\d+\.\s*\*\*(OVERVIEW|KEY FINDINGS|METHODOLOGIES|RECOMMENDATIONS)\*\*[-\s]*(.*?)(?=\d+\.\s*\*\*|\Z)',
                re.DOTALL
            )

            matches = section_pattern.finditer(full_summary)
            for match in matches:
                section_name = match.group(1).lower().replace(' ', '_')
                content = match.group(2).strip()
                if section_name in sections:
                    sections[section_name] = content

            # Generate any missing sections with high detail
            for section in sections:
                if not sections[section]:
                    detail_prompt = f"""Generate an EXTREMELY DETAILED {section.replace('_', ' ')} section for this document.
                    Include all relevant details, examples, and specific information. Document excerpt:\n\n{self.full_text[:20000]}"""
                    response = text_model.generate_content(detail_prompt)
                    sections[section] = response.text

            # Post-process to ensure rich content
            for section, content in sections.items():
                if len(content.split()) < 100:  # If content is too brief
                    enhancement_prompt = f"Expand this {section} section with more details, examples, and analysis:\n\n{content}"
                    enhanced = text_model.generate_content(enhancement_prompt)
                    sections[section] = enhanced.text

            return sections

        except Exception as e:
            print(f"Detailed summary generation failed: {str(e)}")
            # Try to recover with a basic summary
            try:
                basic_summary = text_model.generate_content(
                    f"Generate a comprehensive summary of this document:\n\n{self.full_text[:50000]}"
                )
                return {
                    "overview": basic_summary.text,
                    "key_findings": "See overview for key findings",
                    "methodologies": "See overview for methodologies",
                    "recommendations": "See overview for recommendations"
                }
            except:
                return {
                    "overview": "Error generating summary",
                    "key_findings": "",
                    "methodologies": "",
                    "recommendations": ""
                }

    def generate_tl_dr(self):
        """Generate a very brief executive summary"""
        try:
            if not self.full_text:
                raise ValueError("Document not loaded yet")

            prompt = f"Provide a 3-sentence TL;DR summary of this document:\n\n{self.full_text[:50000]}"
            response = text_model.generate_content(prompt)
            return response.text
        except Exception as e:
            print(f"Error generating TL;DR: {str(e)}")
            return None


# Initialize pipeline
pdf_path = "./2505.18219v1.pdf"

# Replace with your PDF path
rag = RAGPipeline(pdf_path)
# for i in range(1, 6):
#     pdf_path = f"./sample{i}.pdf"
#     rag = RAGPipeline(pdf_path)


# Process documents
if rag.load_and_process_documents():
    # Generate comprehensive summary
    detailed_summary = rag.generate_structured_summary()

    for section, content in detailed_summary.items():
        print(f"\n{'='*40}")
        print(f"{section.upper()}")
        print(f"{'='*40}\n")
        print(content)
    with open("summary.txt", "w", encoding="utf-8") as f:
        for section, content in detailed_summary.items():
            f.write(f"{section.upper()}\n{'='*40}\n{content}\n\n")


"""## Additional Features

You can extend this notebook with:
- Support for other document types (Word, PPT, etc.)
- Web document loading
- Chat interface
- Evaluation metrics
"""


# app = FastAPI()


# class QueryInput(BaseModel):
#     query: str


# @app.post("/rag-query")
# async def rag_query(input: QueryInput):
#     try:
#         response = qa_chain.run(input.query)
#         return {"response": response}
#     except Exception as e:
#         return JSONResponse(content={"error": str(e)}, status_code=500)


# public_url = ngrok.connect(8000)
# print("ðŸš€ Public URL:", public_url)

# uvicorn.run(app, host="0.0.0.0", port=8000)
